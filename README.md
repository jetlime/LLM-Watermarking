# LLM Watermarking

This repository contains prototype code to evaluate a novel technique to identify text written by Large Language Models.

It will illustrate the frequency with which a human chooses a word based on its position in the probability distribution generated by a LLM. The x-axis represents the rank or position of the word chosen by the human in the probability distribution generated by the LLM. For example, based on the previous sequences of tokens in the same document, if the human chose a token that the LLM predicted as the 7th most probable token, it falls into bin 6. This axis shows the number of times the human chose a word from each rank in the LLM's probability distribution. Higher bars indicate a higher frequency of words chosen from that rank. All tokens not present in the log probability distribution returned by the model API, are placed in the last frequency bin.

The current Sequence Window size is of 10, which implies that each iteration considers at most 10 of the previous tokens.

## Get Started

1. Create and activate a Python virtual environment to install all dependencies

```bash
python -m venv ./venv/
source ./venv/bin/activate
pip install -r ./requirements.txt
```

2. Specify your Open AI API key

```bash
echo GPT_API_KEY="<OPENAI_API_KEY>" > .env
```

3. Execute the program by defining the Text file you wish to evaluate and the model for which you want to compare the token selection distribution

```bash
python prototype.py ./data/human_input-small.txt "davinci-002" 10
```

## Examples

### Using ```davinci-002```

Short human-generated text (```./data/human-input-small.txt```):

![](results_davinci-002/plot_human_input-small.png)

- Human generated text (```./data/human-input.txt```):

![](results_davinci-002/plot_human_input.png)

- GPT-4o generated text ( ```./data/gpt4o-input.txt```):

![](results_davinci-002/plot_gpt4o_input.png)

- Davinci-002 generated text ( ```./data/davinci-002-input.txt```):

![](results_davinci-002/plot_davinci-002_input.png)

### Using ```gpt3.5-turbo-1106```

TO BE IMPLEMENTED

## Cost Analysis

![](cost-analysis/cost_per_char_inf_sequence_window.png)
![](cost-analysis/cost_per_char_fixed_sequence_window_10.png)
![](cost-analysis/cost_per_char_fixed_sequence_window_100.png)

## Evaluation Data

Evaluation data is either generated by a human being during the pre-LLM era or by an LLM, as indicated by the suffix of the files located in the ```./data/``` folder.

For instance, ```./data/gpt4o_input.txt```, indicates a text file written by the GPT-4o model.

## Current Limitations to fix:

- [ ] The API endpoint I currently rely on, is not compatible with the newer instruction fine-tuned models (GPT 4 & 3.5).

- [ ] Currently, no obvious visual difference is observed between samples written by humans and LLMs. It may be due to the very small size of the log probability distribution returned by the ```davinci-002``` model (usually only 5). It may also depend on the temperature or the top_p and top_k sampling values. 
